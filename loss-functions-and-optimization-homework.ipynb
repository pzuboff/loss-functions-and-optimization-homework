{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60fd825e",
   "metadata": {},
   "source": [
    "# Описание"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425280d9",
   "metadata": {},
   "source": [
    "В домашнем задании необходимо применить полученные знания в теории оптимизации и машинном обучении для реализации логистической регрессии."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f606df18",
   "metadata": {},
   "source": [
    "## Импорт библиотек"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f3cf1b0-19ba-43b8-8241-c0eb35ad184c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split,  # Импортируем функцию для разделения выборок\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c17f7bb",
   "metadata": {},
   "source": [
    "## Загрузка и подготовка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd8b9322-3e1f-46c4-8056-b009b2dffc6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загрузка данных и отбор нужных классов\n",
    "data = load_iris()\n",
    "# Оставляем только два класса (1 и 2), удаляем класс 0 из набора данных\n",
    "X = data.data[data.target != 0]\n",
    "Y = data.target[data.target != 0] - 1  # Нормализация классов: 1 -> 0, 2 -> 1\n",
    "\n",
    "# Разделение на обучающую и тестовую выборки\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)  # 20% на тестирование"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb162dc",
   "metadata": {},
   "source": [
    "## Создание класса логистической регрессии"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4948fca4-2914-4dff-ae04-9569c1387c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Класс логистической регрессии\n",
    "class My_LogisticRegression:\n",
    "    def __init__(self, learning_rate=0.01, epochs=10000, optimizer='gradient_descent', **kwargs):\n",
    "        self.learning_rate = learning_rate  # Скорость обучения\n",
    "        self.epochs = epochs               # Количество эпох обучения\n",
    "        self.optimizer = optimizer         # Оптимизатор\n",
    "        self.kwargs = kwargs               # Дополнительные параметры\n",
    "        self.theta = None                  # Параметры модели\n",
    "\n",
    "    # Функция активации сигмоида\n",
    "    def sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    # Прогноз вероятностей принадлежности к классу.\n",
    "    def predict_proba(self, X):\n",
    "        return self.sigmoid(np.dot(X, self.theta))\n",
    "\n",
    "    # Прогноз класса на основе вероятности.\n",
    "    def predict(self, X):\n",
    "        return (self.predict_proba(X) >= 0.5).astype(int)\n",
    "\n",
    "    # Функция потерь (логистическая потеря).\n",
    "    def loss(self, h, y):\n",
    "        return (-y * np.log(h) - (1 - y) * np.log(1 - h)).mean()\n",
    "\n",
    "    # Обучение модели.\n",
    "    def fit(self, X, y):\n",
    "        X = np.c_[np.ones(X.shape[0]), X]  # Добавление единичного столбца для сдвига (bias)\n",
    "        self.theta = np.zeros(X.shape[1])  # Инициализация параметров нулями\n",
    "\n",
    "        # Выбор оптимизатора\n",
    "        if self.optimizer == 'gradient_descent':\n",
    "            self._fit_gradient_descent(X, y)\n",
    "        elif self.optimizer == 'rmsprop':\n",
    "            self._fit_rmsprop(X, y)\n",
    "        elif self.optimizer == 'nadam':\n",
    "            self._fit_nadam(X, y)\n",
    "        else:\n",
    "            raise ValueError(\"Unknown optimizer\")\n",
    "\n",
    "    # Градиентный спуск.\n",
    "    def _fit_gradient_descent(self, X, y):\n",
    "        for _ in range(self.epochs):\n",
    "            h = self.predict_proba(X)\n",
    "            gradient = np.dot(X.T, (h - y)) / y.size\n",
    "            self.theta -= self.learning_rate * gradient\n",
    "\n",
    "    # Реализация RMSProp.\n",
    "    def _fit_rmsprop(self, X, y):\n",
    "        epsilon = self.kwargs.get('epsilon', 1e-8)\n",
    "        beta = self.kwargs.get('beta', 0.9)\n",
    "        cache = np.zeros(X.shape[1])\n",
    "\n",
    "        for _ in range(self.epochs):\n",
    "            h = self.predict_proba(X)\n",
    "            gradient = np.dot(X.T, (h - y)) / y.size\n",
    "            cache = beta * cache + (1 - beta) * gradient**2\n",
    "            self.theta -= self.learning_rate * gradient / (np.sqrt(cache) + epsilon)\n",
    "\n",
    "    # Реализация Nadam (Nesterov-accelerated Adam).\n",
    "    def _fit_nadam(self, X, y):\n",
    "        beta1 = self.kwargs.get('beta1', 0.9)\n",
    "        beta2 = self.kwargs.get('beta2', 0.999)\n",
    "        epsilon = self.kwargs.get('epsilon', 1e-8)\n",
    "        m = np.zeros(X.shape[1])\n",
    "        v = np.zeros(X.shape[1])\n",
    "        t = 0\n",
    "\n",
    "        for _ in range(self.epochs):\n",
    "            t += 1\n",
    "            h = self.predict_proba(X)\n",
    "            gradient = np.dot(X.T, (h - y)) / y.size\n",
    "            m = beta1 * m + (1 - beta1) * gradient\n",
    "            v = beta2 * v + (1 - beta2) * gradient**2\n",
    "            m_hat = m / (1 - np.power(beta1, t))\n",
    "            v_hat = v / (1 - np.power(beta2, t))\n",
    "            self.theta -= self.learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c337a42",
   "metadata": {},
   "source": [
    "## Тестрирование"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8337610-6d5d-4efc-97b4-ac3da09db73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Тестирование различных методов оптимизации\n",
    "def run_experiment(optimizer, **kwargs):\n",
    "    model = My_LogisticRegression(optimizer=optimizer, **kwargs)\n",
    "    start_time = datetime.now()  # Время начала\n",
    "    model.fit(X_train, Y_train)  # Обучение модели\n",
    "    end_time = datetime.now()     # Время окончания\n",
    "    \n",
    "    # Рассчет потерь и точности на тестовой выборке\n",
    "    loss = model.loss(model.predict_proba(np.c_[np.ones(X_test.shape[0]), X_test]), Y_test)\n",
    "    accuracy = (model.predict(np.c_[np.ones(X_test.shape[0]), X_test]) == Y_test).mean()\n",
    "    return loss, accuracy, end_time - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40677745-a4d4-4327-88a3-8a0355e01848",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Запуск экспериментов\n",
    "results = []\n",
    "for optimizer in ['gradient_descent', 'rmsprop', 'nadam']:\n",
    "    loss, accuracy, time_taken = run_experiment(optimizer, learning_rate=0.01, epochs=10000, beta=0.9, epsilon=1e-8, beta1=0.9, beta2=0.999)\n",
    "    results.append((optimizer, loss, accuracy, time_taken))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d204420e-3b22-4594-8ff1-c20e1c7f1fdc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              Метод    Потеря  Точность           Время работы\n",
      "0  gradient_descent  0.312467      0.85 0 days 00:00:00.116999\n",
      "1           rmsprop  0.769647      0.85 0 days 00:00:00.147511\n",
      "2             nadam  1.054709      0.85 0 days 00:00:00.214510\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Представление результатов в виде DataFrame\n",
    "df_results = pd.DataFrame(results, columns=['Метод', 'Потеря', 'Точность', 'Время работы'])\n",
    "print(df_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c36a1a8-1ed7-4d3b-a4bd-2db0bd3b3aaf",
   "metadata": {},
   "source": [
    "## Вывод"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c04e70-ccd3-4162-a02e-6b095c7dea82",
   "metadata": {},
   "source": [
    "На основе предоставленного кода и выполненного задания по логистической регрессии с использованием различных методов оптимизации, можно сделать следующие выводы:\n",
    "\n",
    "**Цели эксперимента**: Основная цель заключалась в сравнении эффективности различных методов оптимизации (градиентный спуск, RMSProp и Nadam) для алгоритма логистической регрессии на выборке данных о различных видах ирисов.\n",
    "\n",
    "**Методология**:\n",
    "Датасет был загружен из библиотеки Sklearn, отобраны два класса: Iris Versicolor и Iris Virginica.\n",
    "\n",
    "Логистическая регрессия была реализована вручную в виде класса с методами для обучения и предсказаний.\n",
    "\n",
    "Применялись три различных метода оптимизации, каждый из которых был отдельно протестирован и проанализирован.\n",
    "\n",
    "**Результаты**:\n",
    "\n",
    "В таблице представлены значения потерь и точности для каждого из оптимизаторов:\n",
    "\n",
    "Градиентный спуск: log_loss — 0.312467, accuracy — 0.85\n",
    "\n",
    "RMSProp: log_loss — 0.769647, accuracy — 0.85\n",
    "\n",
    "Nadam: log_loss — 1.054709, accuracy — 0.85\n",
    "\n",
    "Как видно, при использовании градиентного спуска была достигнута наименьшая потеря, что свидетельствует о более быстром и качественном обучении модели. Несмотря на одинаковую точность, разница в значениях потерь указывает на то, что градиентный спуск справляется с этой задачей более эффективно.\n",
    "\n",
    "**Время работы**: Методы оптимизации также показали различия по времени работы:\n",
    "\n",
    "Градиентный спуск: 0.116999 сек.\n",
    "\n",
    "RMSProp: 0.147511 сек.\n",
    "\n",
    "Nadam: 0.214510 сек.\n",
    "\n",
    "Градиентный спуск оказался самым быстрым методом, что делает его предпочтительным выбором в данной ситуации.\n",
    "\n",
    "**Выводы о методах оптимизации**:\n",
    "\n",
    "Градиентный спуск проявил наилучшие результаты по метрикам качества (потеря и скорость) и является рекомендуемым методом для данной задачи.\n",
    "RMSProp и Nadam, несмотря на свои преимущества в других задачах или ситуациях, не показали значительного выигрыша в данной конкретной задаче и их использование не оправдывает затрат времени по сравнению с градиентным спуском."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
